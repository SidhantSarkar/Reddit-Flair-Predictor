{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Flair Prediction Using Saved Models</h1>\n",
    "<h3>Import trained models and Predict</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Python Modules and Dependencies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import pickle\n",
    "import praw\n",
    "import requests \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# NLP PreProcessors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions to Pre-Process Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(inputText):\n",
    "    if(type(inputText)==float):\n",
    "        inputText = ''\n",
    "    inputText = str((inputText.encode('ascii', 'ignore')).decode('utf-8')).lower().split()\n",
    "    specialChars = string.punctuation.replace('#','').replace('+','').replace('_','')\n",
    "    table = str.maketrans('', '', specialChars)\n",
    "    words = [w.translate(table) for w in inputText]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    words = ' '.join(stemmed)\n",
    "    return words\n",
    "\n",
    "def splitUrl(inputText):\n",
    "    inputText = inputText.lower().split('/')\n",
    "    inputText = filter(None, inputText)\n",
    "    inputText = [x for x in inputText if ((x != 'https:') and (x != 'http:'))]\n",
    "    inputText[0] = inputText[0].split('.')\n",
    "    inputText[0] = [x for x in inputText[0] if ((x != 'com') and (x != 'www'))]\n",
    "    inputText[0] = ' '.join(inputText[0])\n",
    "    words = ' '.join(inputText)\n",
    "    return words\n",
    "\n",
    "def classifyTime(inputText):\n",
    "    hours = time.localtime(int(inputText)).tm_hour\n",
    "    if(hours in range(6,12)):\n",
    "        return 'Morning'\n",
    "    elif(hours in range(12, 17)):\n",
    "        return 'Noon'\n",
    "    elif(hours in range(17, 21)):\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extract relevant data from input link given</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPostDetails(redditUrl,dataDictionary):\n",
    "    try:\n",
    "        post = reddit.submission(url=redditUrl)\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?subreddit=india&ids=' + str(post.id)\n",
    "    req = json.loads(requests.get(url).text)\n",
    "    \n",
    "    if(len(req['data']) != 1):\n",
    "        return False\n",
    "    \n",
    "    commentText = ''\n",
    "    post.comments.replace_more(limit=0)\n",
    "    comments = post.comments.list()\n",
    "    for comment in comments:\n",
    "        if(comment.is_root):\n",
    "            commentText += str(comment.body)+' '\n",
    "    \n",
    "    submission = req['data'][0]\n",
    "    dataDictionary['author_fullname'].append(str(submission.setdefault('author_fullname', 'null')))\n",
    "    dataDictionary['created_utc'].append(submission.setdefault('created_utc', 0))\n",
    "    dataDictionary['domain'].append(str(submission.setdefault('domain', 'null')))\n",
    "    dataDictionary['is_crosspostable'].append(submission.setdefault('is_crosspostable', 'false'))\n",
    "    dataDictionary['is_reddit_media_domain'].append(submission.setdefault('is_reddit_media_domain', 'false'))\n",
    "    dataDictionary['post_hint'].append(str(submission.setdefault('post_hint', 'null')))\n",
    "    dataDictionary['num_comments'].append(submission.setdefault('num_comments', 0))\n",
    "    dataDictionary['permalink'].append(str(submission.setdefault('permalink', 'null')))\n",
    "    dataDictionary['score'].append(submission.setdefault('score', 0))\n",
    "    dataDictionary['selftext'].append(str(submission.setdefault('selftext', 'null')))\n",
    "    dataDictionary['title'].append(str(submission.setdefault('title', 'null')))\n",
    "    dataDictionary['url'].append(str(submission.setdefault('url', 'null')))\n",
    "    dataDictionary['comments'].append(str(commentText))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Make Predictions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='CLIENT_ID',\n",
    "                     client_secret='CLIENT_SECRET',\n",
    "                     user_agent='default')\n",
    "\n",
    "dataDictionary = {'author_fullname': [],\n",
    "                  'created_utc' : [],\n",
    "                  'domain' : [],\n",
    "                  'is_crosspostable' : [],\n",
    "                  'is_reddit_media_domain' : [],\n",
    "                  'post_hint' : [],\n",
    "                  'num_comments' : [],\n",
    "                  'permalink' : [],\n",
    "                  'score' : [],\n",
    "                  'selftext' : [],\n",
    "                  'title' : [],\n",
    "                  'url' : [],\n",
    "                  'comments' : []\n",
    "                }\n",
    "    \n",
    "res = getPostDetails('https://www.reddit.com/r/india/comments/cfw/',dataDictionary)\n",
    "if(res != True):\n",
    "#     Handle Error\n",
    "    print('Test')\n",
    "\n",
    "else:\n",
    "    pandasFrame = pd.DataFrame(dataDictionary)\n",
    "\n",
    "    pandasFrame['created_utc'] = pandasFrame['created_utc'].apply(classifyTime)\n",
    "    pandasFrame['domain'] = pandasFrame['domain'].apply(splitUrl)\n",
    "    pandasFrame['post_hint'] = pandasFrame['post_hint'].apply(cleanText)\n",
    "    pandasFrame['permalink'] = pandasFrame['permalink'].apply(splitUrl)\n",
    "    pandasFrame['selftext'] = pandasFrame['selftext'].apply(cleanText)\n",
    "    pandasFrame['title'] = pandasFrame['title'].apply(cleanText)\n",
    "    pandasFrame['url'] = pandasFrame['url'].apply(splitUrl)\n",
    "    pandasFrame['comments'] = pandasFrame['comments'].apply(cleanText)\n",
    "    pandasFrame = pandasFrame.replace(r'^\\s*$', np.nan, regex=True)\n",
    "    pandasFrame = pandasFrame.replace(np.nan, '')\n",
    "\n",
    "    pickle_in_Model = open(\"trainedModel.pickle\",\"rb\")\n",
    "    pickle_in_Vectorizer = open(\"vectorizer.pickle\",\"rb\")\n",
    "    pickle_in_Labels = open(\"labels.pickle\",\"rb\")\n",
    "\n",
    "    model = pickle.load(pickle_in_Model)\n",
    "    tfidVectorizer = pickle.load(pickle_in_Vectorizer)\n",
    "    labels = pickle.load(pickle_in_Labels)\n",
    "\n",
    "    newDF = pd.DataFrame()\n",
    "\n",
    "    for column in pandasFrame.columns:\n",
    "        if(pandasFrame[column].dtype == 'object'):\n",
    "            temp = pd.DataFrame(tfidVectorizer[column].transform(pandasFrame[column]).todense(),columns=tfidVectorizer[column].get_feature_names())\n",
    "            newDF = pd.concat([newDF,temp], axis=1)\n",
    "            pandasFrame = pandasFrame.drop(columns=column)\n",
    "\n",
    "    #  Indexing Problem Resolution\n",
    "    pandasFrame.reset_index(drop=True, inplace=True)\n",
    "    newDF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    pandasFrame = pd.concat([pandasFrame, newDF], axis=1)\n",
    "\n",
    "    prediction = model.predict_proba(pandasFrame)\n",
    "    best_3 = np.flip(np.argsort(prediction, axis=1)[:,-3:] ,1)\n",
    "    prediction = [labels[x] for x in best_3]\n",
    "    print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
