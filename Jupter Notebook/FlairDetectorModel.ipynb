{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reddit Flair Predictor</h1>\n",
    "<h6>A reddit flait predictor made using machine learning algorithms</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Files</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Modules\n",
    "import time\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests \n",
    "import json\n",
    "import praw\n",
    "import pymongo\n",
    "import dns\n",
    "\n",
    "# NLP PreProcessors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Tokenizers, Metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extract Reddit Data</h2>\n",
    "<h4>Extracted 6 Months of Submission Data from r/India</h4>\n",
    "<p>Used push-shift.io API service to collect data</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentTime = int(time.time())\n",
    "prevTime = currentTime - 86400\n",
    "\n",
    "dataDictionary = {'author_flair_text' : [],\n",
    "                  'author_fullname': [],\n",
    "                  'created_utc' : [],\n",
    "                  'domain' : [],\n",
    "                  'is_crosspostable' : [],\n",
    "                  'is_meta' : [],\n",
    "                  'is_original_content' : [],\n",
    "                  'is_reddit_media_domain' : [],\n",
    "                  'is_robot_indexable' : [],\n",
    "                  'is_self' : [],\n",
    "                  'is_video' : [],\n",
    "                  'post_hint' : [],\n",
    "                  'link_flair_text' : [],\n",
    "                  'media_only' : [],\n",
    "                  'num_comments' : [],\n",
    "                  'permalink' : [],\n",
    "                  'score' : [],\n",
    "                  'selftext' : [],\n",
    "                  'title' : [],\n",
    "                  'total_awards_received' : [],\n",
    "                  'url' : [],\n",
    "                  'comments' : []\n",
    "                }\n",
    "\n",
    "reddit = praw.Reddit(client_id='ID',\n",
    "                     client_secret='SECRET',\n",
    "                     user_agent='default')\n",
    "\n",
    "i = 0\n",
    "# 190 days data\n",
    "while i < 190:\n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/?subreddit=india&filter=author_flair_text,author_fullname,created_utc,domain,id,is_crosspostable,is_meta,is_original_content,is_reddit_media_domain,is_robot_indexable,is_self,is_video,post_hint,link_flair_text,media_only,num_comments,permalink,score,selftext,title,total_awards_received,url&size=500&after='+str(prevTime)+'&before='+str(currentTime)+'&sort=desc'\n",
    "    req = json.loads(requests.get(url).text)\n",
    "    for submission in req['data']:\n",
    "        if('link_flair_text' in submission):\n",
    "            commentText = ''\n",
    "            temp = reddit.submission(id=submission['id'])\n",
    "            temp.comments.replace_more(limit=0)\n",
    "            comments = temp.comments.list()\n",
    "            for comment in comments:\n",
    "                if(comment.is_root):\n",
    "                    commentText += str(comment.body)+' '\n",
    "            dataDictionary['author_flair_text'].append(str(submission.setdefault('author_flair_text', 'null')))\n",
    "            dataDictionary['author_fullname'].append(str(submission.setdefault('author_fullname', 'null')))\n",
    "            dataDictionary['created_utc'].append(submission.setdefault('created_utc', 0))\n",
    "            dataDictionary['domain'].append(str(submission.setdefault('domain', 'null')))\n",
    "            dataDictionary['is_crosspostable'].append(submission.setdefault('is_crosspostable', 'false'))\n",
    "            dataDictionary['is_meta'].append(submission.setdefault('is_meta', 'false'))\n",
    "            dataDictionary['is_original_content'].append(submission.setdefault('is_original_content', 'false'))\n",
    "            dataDictionary['is_reddit_media_domain'].append(submission.setdefault('is_reddit_media_domain', 'false'))\n",
    "            dataDictionary['is_robot_indexable'].append(submission.setdefault('is_robot_indexable', 'false'))\n",
    "            dataDictionary['is_self'].append(submission.setdefault('is_self', 'false'))\n",
    "            dataDictionary['is_video'].append(submission.setdefault('is_video', 'false'))\n",
    "            dataDictionary['post_hint'].append(str(submission.setdefault('post_hint', 'null')))\n",
    "            dataDictionary['link_flair_text'].append(str(submission.setdefault('link_flair_text', 'null')))\n",
    "            dataDictionary['media_only'].append(str(submission.setdefault('media_only', 'null')))\n",
    "            dataDictionary['num_comments'].append(submission.setdefault('num_comments', 0))\n",
    "            dataDictionary['permalink'].append(str(submission.setdefault('permalink', 'null')))\n",
    "            dataDictionary['score'].append(submission.setdefault('score', 0))\n",
    "            dataDictionary['selftext'].append(str(submission.setdefault('selftext', 'null')))\n",
    "            dataDictionary['title'].append(str(submission.setdefault('title', 'null')))\n",
    "            dataDictionary['total_awards_received'].append(submission.setdefault('total_awards_received', 0))\n",
    "            dataDictionary['url'].append(str(submission.setdefault('url', 'null')))\n",
    "            dataDictionary['comments'].append(str(commentText))\n",
    "    currentTime = prevTime\n",
    "    prevTime-=86400\n",
    "    i+=1\n",
    "    print('Done Day', i)\n",
    "\n",
    "print(set(dataDictionary['link_flair_text']), len(set(dataDictionary['link_flair_text'])))\n",
    "\n",
    "pandasFrame = pd.DataFrame(dataDictionary)\n",
    "pandasFrame.to_csv('India_data_6months.csv', index=False)\n",
    "\n",
    "pickle_out = open(\"data.pickle\",\"wb\")\n",
    "pickle.dump(dataDictionary, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Insert data into Mongo Atlas Instance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"#\")\n",
    "db = client.python\n",
    "print('Connected')\n",
    "collection = db.data\n",
    "data = pd.read_csv('./India_data_6months.csv')\n",
    "print('Read')\n",
    "collection.insert_many(data.to_dict('records'))\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Helper Functions to Pre-Process Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanText(inputText):\n",
    "    if(type(inputText)==float):\n",
    "        inputText = ''\n",
    "    inputText = str((inputText.encode('ascii', 'ignore')).decode('utf-8')).lower().split()\n",
    "    specialChars = string.punctuation.replace('#','').replace('+','').replace('_','')\n",
    "    table = str.maketrans('', '', specialChars)\n",
    "    words = [w.translate(table) for w in inputText]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    porter = PorterStemmer()\n",
    "    stemmed = [porter.stem(word) for word in words]\n",
    "    words = ' '.join(stemmed)\n",
    "    return words\n",
    "\n",
    "def splitUrl(inputText):\n",
    "    inputText = inputText.lower().split('/')\n",
    "    inputText = filter(None, inputText)\n",
    "    inputText = [x for x in inputText if ((x != 'https:') and (x != 'http:'))]\n",
    "    inputText[0] = inputText[0].split('.')\n",
    "    inputText[0] = [x for x in inputText[0] if ((x != 'com') and (x != 'www'))]\n",
    "    inputText[0] = ' '.join(inputText[0])\n",
    "    words = ' '.join(inputText)\n",
    "    return words\n",
    "\n",
    "def classifyTime(inputText):\n",
    "    hours = time.localtime(int(inputText)).tm_hour\n",
    "    if(hours in range(6,12)):\n",
    "        return 'Morning'\n",
    "    elif(hours in range(12, 17)):\n",
    "        return 'Noon'\n",
    "    elif(hours in range(17, 21)):\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load the saved data into Pandas Frame for use</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pandasFrame = pd.read_csv('India_data_6months.csv')\n",
    "# pandasFrame = pd.read_csv('reddit-india-data1.csv')\n",
    "\n",
    "pandasFrame['created_utc'] = pandasFrame['created_utc'].apply(classifyTime)\n",
    "pandasFrame['domain'] = pandasFrame['domain'].apply(splitUrl)\n",
    "pandasFrame['post_hint'] = pandasFrame['post_hint'].apply(cleanText)\n",
    "pandasFrame['permalink'] = pandasFrame['permalink'].apply(splitUrl)\n",
    "pandasFrame['selftext'] = pandasFrame['selftext'].apply(cleanText)\n",
    "pandasFrame['title'] = pandasFrame['title'].apply(cleanText)\n",
    "pandasFrame['url'] = pandasFrame['url'].apply(splitUrl)\n",
    "pandasFrame['comments'] = pandasFrame['comments'].apply(cleanText)\n",
    "pandasFrame = pandasFrame.replace(r'^\\s*$', np.nan, regex=True)\n",
    "pandasFrame = pandasFrame.replace(np.nan, '')\n",
    "\n",
    "print(pandasFrame.shape)\n",
    "\n",
    "if(len(set(pandasFrame.isnull().any()))==1):\n",
    "    print('Data is Not Empty')\n",
    "\n",
    "Y = pandasFrame['link_flair_text']\n",
    "factorizedLabels = pd.factorize(Y)\n",
    "\n",
    "targets = list(set(Y))\n",
    "pandasFrame = pandasFrame.drop(columns=\"link_flair_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Find Correlation between various features </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidVectorizer = {}\n",
    "\n",
    "X = pandasFrame\n",
    "tempDataFrame = pandasFrame\n",
    "newTest = pd.DataFrame()\n",
    "\n",
    "# Text fields to tf-idf\n",
    "for column in X.columns:\n",
    "    if(X[column].dtype == 'object'):\n",
    "        tfid = TfidfVectorizer()\n",
    "        tfidVectorizer[column] = tfid.fit(tempDataFrame[column])\n",
    "        test = pd.DataFrame(tfidVectorizer[column].transform(tempDataFrame[column]).todense(),columns=tfidVectorizer[column].get_feature_names())\n",
    "        newTest = pd.concat([newTest,test], axis=1)\n",
    "        tempDataFrame = tempDataFrame.drop(columns=column)\n",
    "        \n",
    "tempDataFrame = pd.concat([tempDataFrame, pd.DataFrame({'labels':factorizedLabels[0]})], axis=1)\n",
    "tempDataFrame.corr(method ='pearson')['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Remove Highly Correlated and Redundant Features </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandasFrame = pandasFrame.drop(columns=\"author_fullname\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"author_flair_text\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"is_crosspostable\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"is_meta\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"is_original_content\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"is_robot_indexable\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"is_self\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"is_video\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"media_only\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"permalink\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"total_awards_received\")\n",
    "pandasFrame = pandasFrame.drop(columns=\"post_hint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Divide Data into training and test data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidVectorizer = {}\n",
    "\n",
    "X = pandasFrame\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, factorizedLabels[0], test_size=0.3, random_state = 42)\n",
    "\n",
    "newDFTrain = pd.DataFrame()\n",
    "newDFTest = pd.DataFrame()\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Text fields to tf-idf\n",
    "for column in X.columns:\n",
    "    if(X[column].dtype == 'object'):\n",
    "        print(column)\n",
    "        tfid = TfidfVectorizer(max_df=0.85,stop_words=stop_words,max_features=1500,smooth_idf=True,use_idf=True)\n",
    "        tfidVectorizer[column] = tfid.fit(X_train[column])\n",
    "        train = pd.DataFrame(tfidVectorizer[column].transform(X_train[column]).todense(),columns=tfidVectorizer[column].get_feature_names())\n",
    "        test = pd.DataFrame(tfidVectorizer[column].transform(X_test[column]).todense(),columns=tfidVectorizer[column].get_feature_names())\n",
    "        newDFTrain = pd.concat([newDFTrain,train], axis=1)\n",
    "        newDFTest = pd.concat([newDFTest,test], axis=1)\n",
    "        X_train = X_train.drop(columns=column)\n",
    "        X_test = X_test.drop(columns=column)\n",
    "        \n",
    "#  Indexing Problem Resolution\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "newDFTrain.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "newDFTest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X_train = pd.concat([X_train, newDFTrain], axis=1)\n",
    "X_test = pd.concat([X_test,newDFTest], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Selecting best Classifiers after scaling data to common scale</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = []\n",
    "\n",
    "pipelines.append(('ScaledSGDClassifier', Pipeline([('Scaler', StandardScaler()),('SGDClassifier',SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None))])))\n",
    "pipelines.append(('ScaledLogisticRegression', Pipeline([('Scaler', StandardScaler()),('LogisticRegression',LogisticRegression())])))\n",
    "pipelines.append(('ScaledKNeighborsClassifier', Pipeline([('Scaler', StandardScaler()),('KNeighborsClassifier', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledRandomForestClassifier', Pipeline([('Scaler', StandardScaler()),('RandomForestClassifier', RandomForestClassifier())])))\n",
    "pipelines.append(('ScaledMLPClassifier', Pipeline([('Scaler', StandardScaler()),('MLPClassifier', MLPClassifier())])))\n",
    "pipelines.append(('ScaledDecisionTreeRegressor', Pipeline([('Scaler', StandardScaler()),('DecisionTreeRegressor', DecisionTreeRegressor())])))\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10, random_state=21)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating Training Models and finding the most accurate model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "def bayesClassifier(X_train,X_test,y_train,y_test):\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Naive Bayes Accuracy: ', accuracy)\n",
    "    models.append([accuracy,classifier])\n",
    "    \n",
    "def randomForestClassifier(X_train,X_test,y_train,y_test):\n",
    "    classifier = RandomForestClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Random Forest Accuracy: ', accuracy)\n",
    "    models.append([accuracy,classifier])\n",
    "    \n",
    "def kNeighborsClassifier(X_train,X_test,y_train,y_test):\n",
    "    classifier = KNeighborsClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('K Neighbors Accuracy: ', accuracy)\n",
    "    models.append([accuracy,classifier])\n",
    "    \n",
    "def stochasticGradientClassifier(X_train,X_test,y_train,y_test):\n",
    "    classifier = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Stochastic Gradient Decent Accuracy: ', accuracy)\n",
    "    models.append([accuracy,classifier])\n",
    "    \n",
    "def logisticRegressionClassifier(X_train,X_test,y_train,y_test):\n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy = accuracy_score(y_pred, y_test)\n",
    "    print('Logistic Regression Accuracy: ', accuracy)\n",
    "    models.append([accuracy,classifier])\n",
    "    \n",
    "bayesClassifier(X_train,X_test,y_train,y_test)\n",
    "randomForestClassifier(X_train,X_test,y_train,y_test)\n",
    "kNeighborsClassifier(X_train,X_test,y_train,y_test)\n",
    "stochasticGradientClassifier(X_train,X_test,y_train,y_test)\n",
    "logisticRegressionClassifier(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving the best Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(models,tfidVectorizer):\n",
    "    models.sort(key=lambda x: x[0], reverse=True)\n",
    "    trainedModel = models[0][1]\n",
    "    pickle_out_Model = open(\"trainedModel.pickle\",\"wb\")\n",
    "    pickle_out_Vectorizer = open(\"vectorizer.pickle\", \"wb\")\n",
    "    pickle_out_Label = open(\"labels.pickle\", \"wb\")\n",
    "    pickle.dump(trainedModel, pickle_out_Model)\n",
    "    pickle.dump(tfidVectorizer, pickle_out_Vectorizer)\n",
    "    pickle.dump(factorizedLabels[1], pickle_out_Label)\n",
    "    pickle_out_Model.close()\n",
    "    pickle_out_Vectorizer.close()\n",
    "    pickle_out_Label.close()\n",
    "\n",
    "saveModel(models, tfidVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>References</h3>\n",
    "<p>https://towardsdatascience.com/your-first-kaggle-competition-submission-64da366e48cb</p>\n",
    "<p>https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568</p>\n",
    "<p>https://www.kaggle.com/junkal/selecting-the-best-regression-model</p>\n",
    "<p>https://towardsdatascience.com/natural-language-processing-on-multiple-columns-in-python-554043e05308</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
